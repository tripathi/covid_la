{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import html5lib\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get unincorporated area boundaries (Do only once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from lxml import html \n",
    "import time\n",
    "\n",
    "#Get page of links to each unincorporated area\n",
    "unincorpurl = 'http://maps.latimes.com/neighborhoods/unincorporated/list/page/1/'\n",
    "unincorpresponse = requests.get(unincorpurl)\n",
    "unincorpsoup = BeautifulSoup(unincorpresponse.text, \"html.parser\")\n",
    "\n",
    "#Get hyperlinks\n",
    "alist = unincorpsoup.findAll('a', href=True)\n",
    "\n",
    "#Loop over each unincorporated area\n",
    "count = 0\n",
    "outfile = \"data/unincorporated_latimes.txt\" #Change to .geojson.  Also fix formatting at a later date\n",
    "for link in alist:\n",
    "    if ('Unincorporated' in str(link)):\n",
    "        print('Working on ', link)\n",
    "        download_url = 'http://maps.latimes.com/'+ link['href']\n",
    "        \n",
    "        pageContent=requests.get(download_url) #Use lxml to be able to scrape javascript.         \n",
    "        tree = html.fromstring(pageContent.content)\n",
    "        js=tree.xpath('//*[@id=\"content\"]/div[1]/script/text()') #XPath for the script\n",
    "        result = re.search('features\": \\[(.*)]    };    ', str(js[0].replace('\\n', ''))) #Extract geoJSON feature\n",
    "        \n",
    "        with open(outfile, \"a+\") as text_file:\n",
    "            if count < 1 :\n",
    "                text_file.write('{\\n \"type\": \"FeatureCollection\", \\n \"features\": [\\n') #Untested\n",
    "            text_file.write(result.group(1)+', \\n') #Be sure to go remove the last comma manually and fix the beginning\n",
    "        time.sleep(1) #pause the code for a sec\n",
    "        count = count + 1\n",
    "with open(outfile, \"a+\") as text_file:\n",
    "    text_file.write('REMOVE PRECEEDING COMMA \\n ] }')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Check file output        \n",
    "import geopandas as gpd\n",
    "demog = gpd.read_file('data/unincorporated_latimes.txt')\n",
    "demog.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily scrape of LA County Public Health table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/0408.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7eeaee2f8c7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtitlestring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtitlestring\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.html'\u001b[0m \u001b[0;31m#Uses locally downloaded file from wayback machine to lookup archived site\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'http://publichealth.lacounty.gov/media/Coronavirus/locations.htm'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/0408.html'"
     ]
    }
   ],
   "source": [
    "#Chose what to scrape\n",
    "archive = False\n",
    "titlestring='0408' \n",
    "\n",
    "if archive:     \n",
    "    url = 'data/'+titlestring+'.html' #Uses locally downloaded file from wayback machine to lookup archived site\n",
    "    soup = BeautifulSoup(open(url),\"html.parser\")\n",
    "else:\n",
    "    url ='http://publichealth.lacounty.gov/media/Coronavirus/locations.htm'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#    datetime.today().strftime('%Y-%m-%d')\n",
    "#    titlestring=datetime.today().strftime('%m%d') #Use today's date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_elements = soup.find_all('tr', {'class' : 'blue text-white'})[-1].find_next_siblings()\n",
    "\n",
    "colnames = ['Locations','Total Cases', 'Rate']\n",
    "#for col in soup.find_all('tr')[0].find_all('td'):\n",
    "#    colnames.append(col.text.replace('*', ''))\n",
    "    \n",
    "df = pd.DataFrame(columns=colnames, index = range(0,len(tr_elements)+1)) #To add in total and Pasadena\n",
    "\n",
    "row_marker = 0\n",
    "\n",
    "for row in tr_elements:\n",
    "    column_marker = 0\n",
    "    columns = row.find_all('td')\n",
    "    for column in columns:        \n",
    "        df.iat[row_marker,column_marker] = column.get_text().replace('--', 'suppressed').replace('City of ', '').replace('Unincorporated - ', '').replace('Los Angeles - ','').replace('*','')\n",
    "        column_marker += 1\n",
    "    row_marker+=1\n",
    "\n",
    "#Add total - currently hardcoded\n",
    "column_marker =0 \n",
    "for column in soup.find_all('tr')[1].find_all('td'):\n",
    "\n",
    "    if (column_marker==0):\n",
    "        print('Renaming ', column.get_text(), 'to Total')\n",
    "        df.iat[-1, column_marker] = 'Total'\n",
    "    else:\n",
    "        df.iat[-1, column_marker] = column.get_text()\n",
    "    column_marker += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Pasadena back in \n",
    "column_marker =0\n",
    "pasadenapop = 141371.\n",
    "for column in soup.find_all('tr')[4].find_all('td'):\n",
    "    if(column_marker <2):\n",
    "        print(column.get_text())\n",
    "        df.iat[-2, column_marker] = column.get_text().replace('-','').replace(' ','')\n",
    "        if(column_marker ==1):\n",
    "            df.iat[-2, column_marker+1] = int( column.get_text().replace('-','')) /pasadenapop *100000\n",
    "    else:\n",
    "        print('Skipping:', column.get_text())\n",
    "    column_marker += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Show which locations have duplicates - due to Unincorporated or City of LA\n",
    "grouped = df[df['Locations'].duplicated(keep=False)].groupby('Locations')\n",
    "df[df['Locations'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Failed alt methods:\n",
    "#[['Total Cases', 'Rate']].sum().head(10)\n",
    "#.agg({'Total Cases' : 'sum'}).head(10)#, 'Rate' : lambda x: x.iloc[n]})\n",
    "\n",
    "for name, group in grouped:\n",
    "#    print('Before', group)\n",
    "    localsum = 0\n",
    "    localpop = 0\n",
    "    firstindex = 0\n",
    "    for row_index, row in group.iterrows():\n",
    "        if firstindex==0:\n",
    "            firstindex = row_index\n",
    "        if (('suppressed' not in row['Total Cases']) and (int(row['Total Cases']) >0)):\n",
    "            localsum = localsum + int(row['Total Cases'])\n",
    "            mypop = int(row['Total Cases'])*100000/float(row['Rate'])\n",
    "            localpop = localpop+mypop\n",
    "            \n",
    "#        print(row_index, 'I have ', row['Total Cases'], ' but my city has ', localsum)\n",
    "#        print(row_index, row['Rate']) \n",
    "    if localsum >0:\n",
    "        localrate = localsum*100000/localpop #This overestimates the rates in communities whose data are suppressed or have 0 confirmed cases\n",
    "#        print('ROW INDEX', row_index)\n",
    "        df.iat[row_index,2] = localrate\n",
    "        df.iat[firstindex,2] = localrate\n",
    "    else:\n",
    "        df.iat[row_index,2] = 0\n",
    "        df.iat[firstindex,2] = 0\n",
    "    df.iat[row_index,1] = localsum\n",
    "    df.iat[firstindex,1] = localsum #Note that different arguments for drop_duplicates could avoid needing to update the firstindex versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check that it updated successfully\n",
    "df[df['Locations'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df#.loc[df['Locations'].str.contains('Under Investigation')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other cleanup - this is different once the index has been changed:\n",
    "df.loc[df['Locations'].str.contains('Under Investigation'), 'Locations'] = 'Under Investigation'\n",
    "df.loc[df['Locations'].str.match('Los Angeles'), 'Locations'] = 'Los Angeles - AGGREGATE'\n",
    "df.loc[df['Locations'].str.match('Laboratory Confirmed Cases (LCC)'), 'Locations'] = 'Total'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[df['Locations'].str.contains('AGGREGATE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Locations', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Fix Long Beach and Pasadena\n",
    "column_marker = 0\n",
    "for column in soup.find_all('tr')[3].find_all('td'):\n",
    "    print(df.loc['Long Beach'])\n",
    "    df.at['Long Beach', colnames[column_marker]] = column.get_text()\n",
    "    #    df.iat[df.loc['Long Beach'] , column_marker] = column.get_text()\n",
    "    #    print('Before', df[df['Locations'].str.contains('Long Beach')])\n",
    "    #    df.iat[df.index(df['Locations'].str.contains('Long Beach')).tolist(), column_marker] = column.get_text()\n",
    "    column_marker += 1\n",
    "    #    print('After', df[df['Locations'].str.contains('Long Beach')])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longbeachpop = 467354.\n",
    "df.at['Long Beach',colnames[1]] = soup.find_all('tr')[3].find_all('td')[1].text\n",
    "df.at['Long Beach',colnames[2]] = int(soup.find_all('tr')[3].find_all('td')[1].text)/longbeachpop*100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['Long Beach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['Woodland Hills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out to CSV. Make the format match what covid_la.ipynb expects\n",
    "df.index.names=['city']\n",
    "df.rename(columns={'Locations': 'city', 'Total Cases': 'count', 'Rate': 'rate'}).to_csv(\n",
    "    \"./data/testcovid_\"+titlestring+\".csv\",\n",
    "    index=True,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "#THERE IS A KNOWN BUG that Under Investigation is dropped in archived scrapes! Must be readded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geo_env] *",
   "language": "python",
   "name": "conda-env-geo_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
